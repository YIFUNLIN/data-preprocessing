{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_learning_with_spark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwAKNU5Easmb"
      },
      "source": [
        "# A comprehensive Guide: Machine Learning with Spark\n",
        "\n",
        "Machine learning is getting popular in solving real-world problems in almost every business domain. It helps solve the problems using the data which is often unstructured, noisy, and in huge size. With the increase in data sizes and various sources of data, solving formulating and machine learning problems using standard techniques pose a big challenge. Spark is a distributed processing engine using the MapReduce framework to solve problems related to big data and processing of it.\n",
        "\n",
        "Spark framework has its own machine learning module called MLlib. In this article, I will use pyspark and spark MLlib to demonstrate the use of machine learning using distributed processing. Readers will be able to learn the below concept with real examples.\n",
        "\n",
        "1. Setting up Spark in the Google Colaboratory\n",
        "2. Machine Learning Basic Concepts\n",
        "3. Preprocessing and Data Transformation using Spark\n",
        "4. Spark Clustering with pyspark\n",
        "5. Classification with pyspark\n",
        "6. Regression methods with pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEe66bnJyUas"
      },
      "source": [
        "# Setting up Spark 3.0.1 in the Google Colaboratory\n",
        "\n",
        "As a first step, I configure the google colab runtime with spark installation. For details, readers may read my article [Getting Started Spark 3.0.0 in Google Colab](https://medium.com/analytics-vidhya/getting-started-spark3-0-0-with-google-colab-9796d350d78) om medium. \n",
        "\n",
        "We will install below programs\n",
        "\n",
        "* Java 8\n",
        "* spark-3.0.1\n",
        "* Hadoop3.2 \n",
        "* [Findspark](https://github.com/minrk/findspark)\n",
        "\n",
        "you can install the LATEST version of Spark using below set of commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMTS06C70XiN"
      },
      "source": [
        "# Run below commands\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DuJKdLldqks"
      },
      "source": [
        "## Environment Variable \n",
        "After installing the spark and Java, set the enviroment variables where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzji3m72doJY"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da7M2Wi1jC1c"
      },
      "source": [
        "## Spark Installation test\n",
        "Lets test the installation of spark in our google colab environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y18KVg34jkXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b772e99f-e0de-49bf-d432-34726317bae2"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Test the spark \n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "\n",
        "df.show(3, False)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/session.py:378: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLbeR8NEd9aW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf04ea40-cef8-4692-e31c-12a95504f5fa"
      },
      "source": [
        "# make sure the version of pyspark\n",
        "import pyspark\n",
        "print(pyspark.__version__)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe37dwkcUZ4F"
      },
      "source": [
        "# Machine Learning \n",
        "\n",
        "Once, we have set up the spark in google colab and made sure it is running with the correct version i.e. 3.0.1 in this case, we can start exploring the machine learning API developed on top of Spark. Pyspark is a higher level of API to use spark with python. For this tutorial, I assume the readers have a basic understanding of Machine learning and used SK-Learn for model building and training. Spark MLlib used the same fit and predict structure as in SK-Learn. \n",
        "\n",
        "In order to reproduce the results, I have uploaded the data to my GitHub and can be accessed easily.\n",
        "\n",
        "> Learn by Doing: Use the colab notebook to run it yourself\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6LU2ggyojmw"
      },
      "source": [
        "# Data Preparation and Transformations in Spark\n",
        "\n",
        "This section covers the basic steps involved in transformations of input feature data into the format Machine Learning algorithms accept. We will be covering the transformations coming with the SparkML library. To understand or read more about the available spark transformations in 3.0.3, follow the below link.\n",
        "\n",
        "https://spark.apache.org/docs/3.0.1/ml-features.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGcSHz1H3ZV7"
      },
      "source": [
        "## Normalize Numeric Data\n",
        "\n",
        "MinMaxScaler is one of the favorite classes shipped with most machine learning libraries. It scaled the data between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcxJ9AOFihT4"
      },
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKzJ4FjViv-P"
      },
      "source": [
        "# Create some dummy feature data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),      #寫在同一個分群\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCCGoUFGjopG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845f9bad-d1bf-43cb-ad9e-c5f351800104"
      },
      "source": [
        "features_df.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+\n",
            "| id|          features|\n",
            "+---+------------------+\n",
            "|  1|[10.0,10000.0,1.0]|\n",
            "|  2|[20.0,30000.0,2.0]|\n",
            "|  3|[30.0,40000.0,3.0]|\n",
            "+---+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMrx6ok3j5QV"
      },
      "source": [
        "# Apply MinMaxScaler transformation                           #資料轉換\n",
        "features_scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"sfeatures\")   #該欄位最大設1最小設0，其餘做等比例縮放\n",
        "smodel = features_scaler.fit(features_df)                        #再利用算距離公式去計算                    \n",
        "sfeatures_df = smodel.transform(features_df)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94487nHqlB36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d59dbc38-8579-46a2-a609-66e86b64e123"
      },
      "source": [
        "sfeatures_df.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|           sfeatures|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|           (3,[],[])|\n",
            "|  2|[20.0,30000.0,2.0]|[0.5,0.6666666666...|\n",
            "|  3|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0fpo-2aoSR6"
      },
      "source": [
        "## Standardize Numeric Data\n",
        "\n",
        "StandardScaler is another well-known class written with machine learning libraries. It normalizes the data between -1 and 1 and converts the data into bell-shaped data. You can demean the data and scale to some variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqzgOUyAoeGJ"
      },
      "source": [
        "from pyspark.ml.feature import  StandardScaler       #Z-score\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgD9wMW9lslE"
      },
      "source": [
        "# Create the dummy data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAgTOmAkl0uV"
      },
      "source": [
        "# Apply the StandardScaler model\n",
        "features_stand_scaler = StandardScaler(inputCol = \"features\", outputCol = \"sfeatures\", withStd=True, withMean=True)\n",
        "stmodel = features_stand_scaler.fit(features_df)\n",
        "stand_sfeatures_df = stmodel.transform(features_df)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT1eT52ymYHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f01a2d-984f-403b-c99d-b36dfc012a6b"
      },
      "source": [
        "stand_sfeatures_df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|           sfeatures|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|\n",
            "|  2|[20.0,30000.0,2.0]|[0.0,0.2182178902...|\n",
            "|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mcttwMxoIBX"
      },
      "source": [
        "## Bucketize Numeric Data\n",
        "\n",
        "The real data sets come with various ranges and sometimes it is advisable to transform the data into well-defined buckets before plugging into machine learning algorithms.\n",
        "\n",
        "Bucketizer class is handy to transform the data into various buckets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmdLCd_woCDM"
      },
      "source": [
        "from pyspark.ml.feature import  Bucketizer         #分區段\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSnGOTVPmn9w"
      },
      "source": [
        "# Define the splits for buckets\n",
        "splits = [-float(\"inf\"), -10, 0.0, 10, float(\"inf\")]\n",
        "b_data = [(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]\n",
        "b_df = spark.createDataFrame(b_data, [\"features\"])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3teBrblnT2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "694e47f8-9e5f-4453-d848-9db6f3e294c9"
      },
      "source": [
        "b_df.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|features|\n",
            "+--------+\n",
            "|  -800.0|\n",
            "|   -10.5|\n",
            "|    -1.7|\n",
            "|     0.0|\n",
            "|     8.2|\n",
            "|    90.1|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LQA_zZtnWlN"
      },
      "source": [
        "# Transforming data into buckets\n",
        "bucketizer = Bucketizer(splits=splits, inputCol= \"features\", outputCol=\"bfeatures\")\n",
        "bucketed_df = bucketizer.transform(b_df)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb3bcGkVntXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4465e09c-7690-40f8-b704-f08c1c4ac4f2"
      },
      "source": [
        "bucketed_df.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+\n",
            "|features|bfeatures|\n",
            "+--------+---------+\n",
            "|  -800.0|      0.0|\n",
            "|   -10.5|      0.0|\n",
            "|    -1.7|      1.0|\n",
            "|     0.0|      2.0|\n",
            "|     8.2|      2.0|\n",
            "|    90.1|      3.0|\n",
            "+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsnnhLDkorcE"
      },
      "source": [
        "## Tokenize text Data\n",
        "\n",
        "Natural Language Processing is one of the main applications of Machine learning. One of the first steps for NLP is tokenizing the text into words or token. We can utilize the Tokenizer class with SparkML to perform this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tDc75dto1SH"
      },
      "source": [
        "from pyspark.ml.feature import  Tokenizer"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kxgayjio6bq"
      },
      "source": [
        "sentences_df = spark.createDataFrame([\n",
        "    (1, \"This is an introduction to sparkMlib\"),\n",
        "    (2, \"Mlib incluse libraries fro classfication and regression\"),\n",
        "    (3, \"It also incluses support for data piple lines\"),\n",
        "    \n",
        "], [\"id\", \"sentences\"])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Jm65OppYsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b692d7ca-61e4-463b-f29f-9a9af691fee0"
      },
      "source": [
        "sentences_df.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|           sentences|\n",
            "+---+--------------------+\n",
            "|  1|This is an introd...|\n",
            "|  2|Mlib incluse libr...|\n",
            "|  3|It also incluses ...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_Jv-WFbpzhz"
      },
      "source": [
        "sent_token = Tokenizer(inputCol = \"sentences\", outputCol = \"words\")\n",
        "sent_tokenized_df = sent_token.transform(sentences_df)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpVwcI4WqHPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123f11e7-22c2-48d9-9302-c0936236a784"
      },
      "source": [
        "sent_tokenized_df.take(10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentences='This is an introduction to sparkMlib', words=['this', 'is', 'an', 'introduction', 'to', 'sparkmlib']),\n",
              " Row(id=2, sentences='Mlib incluse libraries fro classfication and regression', words=['mlib', 'incluse', 'libraries', 'fro', 'classfication', 'and', 'regression']),\n",
              " Row(id=3, sentences='It also incluses support for data piple lines', words=['it', 'also', 'incluses', 'support', 'for', 'data', 'piple', 'lines'])]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF37QTFoqby6"
      },
      "source": [
        "##TF-IDF\n",
        "Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus. Using the above-tokenized data, Let us apply the TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kproZrg-qfLt"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF           #字出現的次數"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZir1QWWqyri"
      },
      "source": [
        "hashingTF = HashingTF(inputCol = \"words\", outputCol = \"rawfeatures\", numFeatures = 20)\n",
        "sent_fhTF_df = hashingTF.transform(sent_tokenized_df)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzzwucNQrVEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9a246f-0bfa-495c-f613-59cb5b65ada9"
      },
      "source": [
        "sent_fhTF_df.take(1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentences='This is an introduction to sparkMlib', words=['this', 'is', 'an', 'introduction', 'to', 'sparkmlib'], rawfeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0}))]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WWlxVe8rezv"
      },
      "source": [
        "idf = IDF(inputCol = \"rawfeatures\", outputCol = \"idffeatures\")\n",
        "idfModel = idf.fit(sent_fhTF_df)\n",
        "tfidf_df = idfModel.transform(sent_fhTF_df)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldrtqjSTr8ZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8309a403-e984-4c8d-a9ed-dc048b5b8613"
      },
      "source": [
        "tfidf_df.take(1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentences='This is an introduction to sparkMlib', words=['this', 'is', 'an', 'introduction', 'to', 'sparkmlib'], rawfeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0}), idffeatures=SparseVector(20, {6: 0.5754, 8: 0.6931, 9: 0.0, 10: 0.6931, 13: 0.2877}))]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nILjvBXDhszw"
      },
      "source": [
        "> User can play with various transformations depending on the requirements of the problem in-hand. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNo8k78Os2si"
      },
      "source": [
        "# Clustering Using PySpark\n",
        "\n",
        "Clustering is a machine learning technique where the data is grouped into a reasonable number of classes using the input features. In this section, we study the basic application of clustering techniques using the spark ML framework. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDFkM7hQs-5T"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
        "import glob"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qvl20sVjlsg"
      },
      "source": [
        "# Downloading the clustering dataset\n",
        "!wget -q 'https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/clustering_dataset.csv'"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtPdJC2bkJNH"
      },
      "source": [
        "Load the clustering data stored in csv format using spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTXI1_UPtUhp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "1e4528b4-284e-49d6-cf0f-2c0e4151089d"
      },
      "source": [
        "# Read the data.\n",
        "clustering_file_name ='clustering_dataset.csv'\n",
        "import pandas as pd\n",
        "# df = pd.read_csv(clustering_file_name)\n",
        "cluster_df = spark.read.csv(clustering_file_name, header=True,inferSchema=True)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-c763ecd7ae56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df = pd.read_csv(clustering_file_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcluster_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/clustering_dataset.csv;"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouHhNCsYkSST"
      },
      "source": [
        "Convert the tabular data into vectorized format using `VectorAssembler` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7J03PqWtUsF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "f4297ade-081d-4c44-f82b-c7d090273efb"
      },
      "source": [
        "# Coverting the input data into features column\n",
        "vectorAssembler = VectorAssembler(inputCols = ['col1', 'col2', 'col3'], outputCol = \"features\")\n",
        "vcluster_df = vectorAssembler.transform(cluster_df)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-01a090015199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Coverting the input data into features column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvectorAssembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'col1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'col2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'col3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvcluster_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorAssembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cluster_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fhb3cHvvlcb"
      },
      "source": [
        "vcluster_df.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVv_Z4mukgrW"
      },
      "source": [
        "Once the data is prepared into the format MLlib can use for models, now we can define and train the clustering algorithm such as K-Means. We can define the number of clusters and initialize the seed as done below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIpipYg1tUve"
      },
      "source": [
        "# Applying the k-means algorithm              #分群方法\n",
        "kmeans = KMeans().setK(3)\n",
        "kmeans = kmeans.setSeed(1)\n",
        "kmodel = kmeans.fit(vcluster_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Ny5xyzk_rI"
      },
      "source": [
        "After training has been finished, let us print the centers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut2LEbdXuveX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f2ad0f-777f-4dda-8090-df685bb1f68c"
      },
      "source": [
        "centers = kmodel.clusterCenters()\n",
        "print(\"The location of centers: {}\".format(centers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The location of centers: [array([35.88461538, 31.46153846, 34.42307692]), array([80.        , 79.20833333, 78.29166667]), array([5.12, 5.84, 4.84])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed = kmodel.transform(vcluster_df).select('features', 'prediction')\n",
        "rows = transformed.collect()\n",
        "print(rows[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub5Fs9VC_mQI",
        "outputId": "d85ce9f3-c12f-41f5-dab3-d49f4ec32644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(features=DenseVector([7.0, 4.0, 1.0]), prediction=2), Row(features=DenseVector([7.0, 7.0, 9.0]), prediction=2), Row(features=DenseVector([7.0, 9.0, 6.0]), prediction=2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie_1Xr5jx01T"
      },
      "source": [
        "# Classification Using PySpark\n",
        "\n",
        "Classification is one of the widely used Machine algorithms and almost every data engineer and data scientist must know about these algorithms. Once the data is loaded and prepared, I will demonstrate three classification algorithms.\n",
        "\n",
        "1. NaiveBayes Classification\n",
        "2. Multi-Layer Perceptron Classification\n",
        "3. Decision Trees Classification\n",
        "\n",
        "\n",
        "We explore the supervised classification algorithms using [IRIS data]( https://archive.ics.uci.edu/ml/datasets/iris). I have uploaded the data into my GitHub to reproduce the results. Users can download the data using below command.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foVc6Tm-3dYk"
      },
      "source": [
        "## Preprocessing the Iris Data\n",
        "\n",
        "In this section, we will be using the IRIS data to understand the classification. To perform ML models, we apply the preprocessing step on our input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grXJWKRwxQBT"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import  StringIndexer\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5jGQaU8xP6A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3767cfa9-f8c5-4321-9f82-26b86ea2b7b6"
      },
      "source": [
        "# Read the iris data\n",
        "df_iris = pd.read_csv(r\"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv\", header=None)\n",
        "iris_df = spark.createDataFrame(df_iris)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-c2c72a2a7552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the iris data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_iris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0miris_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_iris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 503: first byte timeout"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBuUFzZurZv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6806c641-d7a8-40cf-e34a-76deb6c11efa"
      },
      "source": [
        "iris_df.show(5, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+---+-----------+\n",
            "|0  |1  |2  |3  |4          |\n",
            "+---+---+---+---+-----------+\n",
            "|5.1|3.5|1.4|0.2|Iris-setosa|\n",
            "|4.9|3.0|1.4|0.2|Iris-setosa|\n",
            "|4.7|3.2|1.3|0.2|Iris-setosa|\n",
            "|4.6|3.1|1.5|0.2|Iris-setosa|\n",
            "|5.0|3.6|1.4|0.2|Iris-setosa|\n",
            "+---+---+---+---+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk2z99ic0Mg6"
      },
      "source": [
        "# Rename the columns\n",
        "iris_df = iris_df.select(col(\"0\").alias(\"sepal_length\"),\n",
        "                         col(\"1\").alias(\"sepal_width\"),\n",
        "                         col(\"2\").alias(\"petal_length\"),\n",
        "                         col(\"3\").alias(\"petal_width\"),\n",
        "                         col(\"4\").alias(\"species\"),\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJN6zt5g0Mos"
      },
      "source": [
        "# Converting the columns into features\n",
        "vectorAssembler = VectorAssembler(inputCols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
        "                                  outputCol = \"features\")\n",
        "viris_df = vectorAssembler.transform(iris_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgmRVT1M0MzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e591a1-3d61-4a51-dd75-43b4b1d5ca31"
      },
      "source": [
        "viris_df.show(5, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "|5.1         |3.5        |1.4         |0.2        |Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
            "|4.9         |3.0        |1.4         |0.2        |Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
            "|4.7         |3.2        |1.3         |0.2        |Iris-setosa|[4.7,3.2,1.3,0.2]|\n",
            "|4.6         |3.1        |1.5         |0.2        |Iris-setosa|[4.6,3.1,1.5,0.2]|\n",
            "|5.0         |3.6        |1.4         |0.2        |Iris-setosa|[5.0,3.6,1.4,0.2]|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cguoNqa30M4k"
      },
      "source": [
        "indexer = StringIndexer(inputCol=\"species\", outputCol = \"label\")\n",
        "iviris_df = indexer.fit(viris_df).transform(viris_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo72QJfT27c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21636e71-988c-4f66-e04b-4193aaa13f32"
      },
      "source": [
        "iviris_df.show(2, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |label|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "|5.1         |3.5        |1.4         |0.2        |Iris-setosa|[5.1,3.5,1.4,0.2]|0.0  |\n",
            "|4.9         |3.0        |1.4         |0.2        |Iris-setosa|[4.9,3.0,1.4,0.2]|0.0  |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZWwghrd3oT1"
      },
      "source": [
        "## Naive Bayes Calssification\n",
        "\n",
        "Once the data is prepared, we are ready to apply the first classification algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tou1Tk0o27ky"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of8-W2PJ27nQ"
      },
      "source": [
        "# Create the traing and test splits\n",
        "splits = iviris_df.randomSplit([0.6,0.4], 1)\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFdrN8lU4fM2"
      },
      "source": [
        "# Apply the Naive bayes classifier\n",
        "nb = NaiveBayes(modelType=\"multinomial\")\n",
        "nbmodel = nb.fit(train_df)\n",
        "predictions_df = nbmodel.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TccH0PnU4fTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5412710-e3f0-4b19-f4b7-6292b94d3dce"
      },
      "source": [
        "predictions_df.show(1, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |label|rawPrediction                                               |probability                                                 |prediction|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+\n",
            "|4.3         |3.0        |1.1         |0.1        |Iris-setosa|[4.3,3.0,1.1,0.1]|0.0  |[-9.966434726497221,-11.294595492758821,-11.956012812323921]|[0.7134106367667451,0.18902823898426235,0.09756112424899269]|0.0       |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roOzUt5q5F45"
      },
      "source": [
        "Let us Evaluate the trained classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QO9iPYl5Dge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d26c27-2857-42fc-f260-699860846eea"
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "nbaccuracy = evaluator.evaluate(predictions_df)\n",
        "nbaccuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8275862068965517"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxAru6v05jY4"
      },
      "source": [
        "## Multilayer Perceptron Classification\n",
        "\n",
        "The second classifier we will be investigating is a Multi-layer perceptron. In this tutorial, I am not going into details of the optimal MLP network for this problem however in practice, you research the optimal network suitable to the problem in hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrFeWep35qJC"
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thL3QCLF5qMJ"
      },
      "source": [
        "# Define the MLP Classifier\n",
        "layers = [4,5,5,3]\n",
        "mlp = MultilayerPerceptronClassifier(layers = layers, seed=1)\n",
        "mlp_model = mlp.fit(train_df)\n",
        "mlp_predictions = mlp_model.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SqXIlpQ5qYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71905b89-ce0f-48a0-d491-8e071584f8e7"
      },
      "source": [
        "# Evaluate the MLP classifier\n",
        "mlp_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "mlp_accuracy = mlp_evaluator.evaluate(mlp_predictions)\n",
        "mlp_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9827586206896551"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGwfdn_e7CuL"
      },
      "source": [
        "## Decision Trees Classification\n",
        "\n",
        "Another common classifier in the ML family is the Decision Tree Classifier, in this section, we explore this classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0iWDg9v5qmq"
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTx4I55h5qpf"
      },
      "source": [
        "# Define the DT Classifier \n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "dt_model = dt.fit(train_df)\n",
        "dt_predictions = dt_model.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7gN5hXn5qsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad7badc-bf8a-4014-c894-cf1360b1fda8"
      },
      "source": [
        "# Evaluate the DT Classifier\n",
        "dt_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "dt_accuracy = dt_evaluator.evaluate(dt_predictions)\n",
        "dt_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9827586206896551"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIr1nhag7-BC"
      },
      "source": [
        "# Regression using PySpark\n",
        "\n",
        "In this section, we explore the Machine learning models for regression problems using pyspark.Regression models are helpful in predicting the future values using the past data. \n",
        "\n",
        "We will use the [Combined Cycle Power Plant](https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant) data set to predict the net hourly electrical output (EP). I have uploaded the data to my GitHub so that users can reproduce the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8KB4WCu5qWD"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBckB0VJu7q_"
      },
      "source": [
        "# Read the iris data\n",
        "df_ccpp = pd.read_csv(\"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/ccpp.csv\")\n",
        "pp_df = spark.createDataFrame(df_ccpp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK_cSfju5qOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2eaa15-94e2-46ca-ef7f-e5d2cee14e2f"
      },
      "source": [
        "pp_df.show(2, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------+-----+------+\n",
            "|AT   |V    |AP     |RH   |PE    |\n",
            "+-----+-----+-------+-----+------+\n",
            "|14.96|41.76|1024.07|73.17|463.26|\n",
            "|25.18|62.96|1020.04|59.08|444.37|\n",
            "+-----+-----+-------+-----+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukpwadf1-zpI"
      },
      "source": [
        "# Create the feature column using VectorAssembler class\n",
        "vectorAssembler = VectorAssembler(inputCols =[\"AT\", \"V\", \"AP\", \"RH\"], outputCol = \"features\")\n",
        "vpp_df = vectorAssembler.transform(pp_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPwTYHjl-zy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf779463-7134-488e-b48b-b1e9dbfabee8"
      },
      "source": [
        "vpp_df.show(2, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|AT   |V    |AP     |RH   |PE    |features                   |\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024.07,73.17]|\n",
            "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020.04,59.08]|\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhLBqPGUZ3jH"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "We start with simplest regression technique i.e. Linear Regression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scO6fex0-z62"
      },
      "source": [
        "# Define and fit Linear Regression\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"PE\")\n",
        "lr_model = lr.fit(vpp_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQxcXMeB-0BQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c8e8d5-c616-40c8-d648-631ef0852aae"
      },
      "source": [
        "# Print and save the Model output\n",
        "lr_model.coefficients\n",
        "lr_model.intercept\n",
        "lr_model.summary.rootMeanSquaredError"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.557126016749486"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbAUizWd-zsC"
      },
      "source": [
        "#lr_model.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6TgFsHaAF6-"
      },
      "source": [
        "## Decision Tree Regression\n",
        "\n",
        "In thsi section, we explore the Decision Tree Regression commonly used in Machine learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhYf7s5p-zmR"
      },
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t77YAs-yV-GY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c666d6c5-f26d-47c3-d6d4-ed222216e530"
      },
      "source": [
        "vpp_df.show(2, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|AT   |V    |AP     |RH   |PE    |features                   |\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024.07,73.17]|\n",
            "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020.04,59.08]|\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUHK17ZfW5QF"
      },
      "source": [
        "# Define train and test data split\n",
        "splits = vpp_df.randomSplit([0.7,0.3])\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrlTie1sV-Jr"
      },
      "source": [
        "# Define the Decision Tree Model \n",
        "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"PE\")\n",
        "dt_model = dt.fit(train_df)\n",
        "dt_predictions = dt_model.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-wJEvSZaC3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9409ff1-6225-499a-e90f-67e3b12e8974"
      },
      "source": [
        "dt_predictions.show(1, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------+-----+------+-------------------------+------------------+\n",
            "|AT |V    |AP     |RH   |PE    |features                 |prediction        |\n",
            "+---+-----+-------+-----+------+-------------------------+------------------+\n",
            "|2.8|39.64|1011.01|82.96|482.66|[2.8,39.64,1011.01,82.96]|485.93864197530854|\n",
            "+---+-----+-------+-----+------+-------------------------+------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nRcmpivV-pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ec860f-5214-40a3-db54-697a3efb0cfe"
      },
      "source": [
        "# Evaluate the Model\n",
        "dt_evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "dt_rmse = dt_evaluator.evaluate(dt_predictions)\n",
        "print(\"The RMSE of Decision Tree regression Model is {}\".format(dt_rmse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The RMSE of Decision Tree regression Model is 4.558696302748363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpbIb8DSaeSq"
      },
      "source": [
        "## Gradient Boosting Decision Tree Regression\n",
        "\n",
        "Gradient Boosting is another common choice among ML professionals. Let us try the GBM in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAv99jLyV-e5"
      },
      "source": [
        "from pyspark.ml.regression import GBTRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njma6SqaV-Ye"
      },
      "source": [
        "# Define the GBT Model\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"PE\")\n",
        "gbt_model = gbt.fit(train_df)\n",
        "gbt_predictions = gbt_model.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDDGMtZtV-V0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8bec923-9db9-40ca-cee7-96efde8cdbd5"
      },
      "source": [
        "# Evaluate the GBT Model\n",
        "gbt_evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
        "print(\"The RMSE of GBT Tree regression Model is {}\".format(gbt_rmse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The RMSE of GBT Tree regression Model is 4.108481483898151\n"
          ]
        }
      ]
    }
  ]
}